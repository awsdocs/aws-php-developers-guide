.. Copyright 2010-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.

   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0
   International License (the "License"). You may not use this file except in compliance with the
   License. A copy of the License is located at http://creativecommons.org/licenses/by-nc-sa/4.0/.

   This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
   either express or implied. See the License for the specific language governing permissions and
   limitations under the License.

###########################################################
Using |S3| Multipart Uploads with AWS SDK for PHP version 3 
###########################################################

.. meta::
   :description: Break larger files into smaller parts when you upload to Amazon S3 using the AWS SDK for PHP version 3 .
   :keywords: Amazon S3, AWS SDK for PHP version 3  examples, S3 for PHP code examples


With a single ``PutObject`` operation, you can upload objects up to 5 GB in
size. However, by using the multipart upload methods (for example, ``CreateMultipartUpload``,
``UploadPart``, ``CompleteMultipartUpload``, ``AbortMultipartUpload``), you can
upload objects from 5 MB to 5 TB in size.

The following example shows how to:

* Upload an object to |S3|, using :aws-php-class:`ObjectUploader <class-Aws.S3.ObjectUploader.html>`.
* Create a multipart upload for an |S3| object using :aws-php-class:`MultipartUploader <class-Aws.S3.MultipartUploader.html>`.
* Copy objects from one |S3| location to another using :aws-php-class:`ObjectCopier <class-Aws.S3.ObjectCopier.html>`.

.. include:: text/git-php-examples.txt

Object Uploader
================

If you're not sure whether ``PutObject`` or ``MultipartUploader`` is best for the task, use ``ObjectUploader``. ``ObjectUploader`` uploads a large file to |S3| using either ``PutObject`` or ``MultipartUploader``, depending on what is best based on the payload size. 

.. literalinclude::  s3.php.objectuploader.import.txt
   :language: PHP

**Sample Code**

.. literalinclude:: s3.php.objectuploader.main.txt
   :language: PHP
   
MultipartUploader
=================

Multipart uploads are designed to improve the upload experience for larger
objects. They enable you to upload objects in parts
independently, in any order, and in parallel.

|S3| customers are encouraged to use multipart uploads for objects greater
than 100 MB.

MultipartUploader Object
========================

The SDK has a special ``MultipartUploader`` object that simplifies the multipart upload
process.

**Imports**

.. literalinclude::  s3.php.multipart_upload.import.txt
   :language: PHP

**Sample Code**

.. literalinclude:: s3.php.multipart_upload.main.txt
   :language: PHP
   
The uploader creates a generator of part data, based on the provided source and
configuration, and attempts to upload all parts. If some part uploads fail, the
uploader keeps track of them and continues to upload later parts until the entire
source data is read. It then either completes the upload or throws an
exception that contains information about the parts that failed to upload.

Customizing a Multipart Upload
==============================

You can set custom options on the ``CreateMultipartUpload``, ``UploadPart``, and
``CompleteMultipartUpload`` operations executed by the multipart uploader via
callbacks passed to its constructor.

**Imports**

.. literalinclude::  s3.php.multipart_upload_custom.import.txt
   :language: PHP

**Sample Code**

.. literalinclude:: s3.php.multipart_upload_custom.main.txt
   :language: PHP

Manual Garbage Collection Between Part Uploads
----------------------------------------------

If you are hitting the memory limit with large uploads, this may be due to cyclic
references generated by the SDK not yet having been collected by the
`PHP garbage collector <https://www.php.net/manual/en/features.gc.php/>`_ when
your memory limit was hit. Manually invoking the collection algorithm between
operations may allow the cycles to be collected before hitting that limit. The
following example invokes the collection algorithm using a callback before each
part upload. Note that invoking the garbage collector does come with a performance
cost, and optimal usage will depend on your use case and environment.

.. code-block:: php

   $uploader = new MultipartUploader($client, $source, [
      'bucket' => 'your-bucket,
      'key' => 'your-key',
      'before_upload' => function(\Aws\Command $command) {
         gc_collect_cycles();
      }
   ]);

Recovering from Errors
======================

When an error occurs during the multipart upload process, a
``MultipartUploadException`` is thrown. This exception provides access to the
``UploadState`` object, which contains information about the multipart upload's
progress. The ``UploadState`` can be used to resume an upload that failed to
complete.

**Imports**

.. literalinclude::  s3.php.multipart_upload_errors.import.txt
   :language: PHP

**Sample Code**

.. literalinclude:: s3.php.multipart_upload_errors.main.txt
   :language: PHP

Resuming an upload from an ``UploadState`` attempts to upload parts
that are not already uploaded. The state object keeps track of missing parts,
even if they are not consecutive. The uploader reads or seeks through the
provided source file to the byte ranges that belong to the parts that still need
to be uploaded.

``UploadState`` objects are serializable, so you can also resume an
upload in a different process. You can also get the ``UploadState`` object, even
when you're not handling an exception, by calling ``$uploader->getState()``.

.. important::

    Streams passed in as a source to a ``MultipartUploader`` are not
    automatically rewound before uploading. If you're using a stream instead of a
    file path in a loop similar to the previous example, you need to reset the
    ``$source`` variable inside of the ``catch`` block.

**Imports**

.. literalinclude::  s3.php.multipart_upload_stream.import.txt
   :language: PHP

**Sample Code**

.. literalinclude:: s3.php.multipart_upload_stream.main.txt
   :language: PHP

Aborting a Multipart Upload
---------------------------

Sometimes, you might not want to resume an upload, and would rather
abort the the whole thing when an error occurs. This is also easy using the
data contained in the ``UploadState`` object.

.. code-block:: php

    try {
        $result = $uploader->upload();
    } catch (MultipartUploadException $e) {
        // State contains the "Bucket", "Key", and "UploadId"
        $params = $e->getState()->getId();
        $result = $s3Client->abortMultipartUpload($params);
    }

Asynchronous Multipart Uploads
==============================

Calling ``upload()`` on the ``MultipartUploader`` is a blocking request. If you are
working in an asynchronous context, you can get a :doc:`promise <guide_promises>`
for the multipart upload.

.. literalinclude::  s3.php.multipart_upload_async.import.txt
   :language: PHP

**Sample Code**

.. literalinclude:: s3.php.multipart_upload_async.main.txt
   :language: PHP

Configuration
=============

The ``MultipartUploader`` object constructor accepts the following arguments:

``$client``
    The ``Aws\ClientInterface`` object to use for performing the transfers.
    This should be an instance of ``Aws\S3\S3Client``.

``$source``
    The source data being uploaded. This can be a path or URL (for example,
    ``/path/to/file.jpg``), a resource handle (for example, ``fopen('/path/to/file.jpg', 'r)``),
    or an instance of a :aws-php-class:`PSR-7 stream </class-Psr.Http.Message.StreamInterface.html>`.

``$config``
    An associative array of configuration options for the multipart upload.

    The following configuration options are valid:
    
    **acl**
        (``string``) Access control list (ACL) to set on the object being upload. Objects are private by
        default.
    **before_complete**
        (``callable``) Callback to invoke before the ``CompleteMultipartUpload``
        operation. The callback should have a function signature like
        ``function (Aws\Command $command) {...}``.
    **before_initiate**
        (``callable``) Callback to invoke before the ``CreateMultipartUpload``
        operation. The callback should have a function signature like
        ``function (Aws\Command $command) {...}``.
    **before_upload**
        (``callable``) Callback to invoke before any ``UploadPart`` operations. The
        callback should have a function signature like
        ``function (Aws\Command $command) {...}``.
    **bucket**
        (``string``, *required*) Name of the bucket to which the object is being uploaded.
    **concurrency**
        (``int``, *default*: ``int(5)``) Maximum number of concurrent ``UploadPart``
        operations allowed during the multipart upload.
    **key**
        (``string``, *required*) Key to use for the object being uploaded.
    **part_size**
        (``int``, *default*: ``int(5242880)``) Part size, in bytes, to use when doing a
        multipart upload. This must between 5 MB and 5 GB, inclusive.
    **state**
        (``Aws\Multipart\UploadState``) An object that represents the state of the
        multipart upload and that is used to resume a previous upload. When this
        option is provided, the ``bucket``, ``key``, and ``part_size`` options
        are ignored.
    
Multipart Copies
================

The |sdk-php| also includes a ``MultipartCopy`` object that is used in a similar way
to the ``MultipartUploader``, but is designed for copying objects between 5 GB and
5 TB in size within |S3|.

.. literalinclude::  s3.php.multipart_upload_copy.import.txt
   :language: PHP

**Sample Code**

.. literalinclude:: s3.php.multipart_upload_copy.main.txt
   :language: PHP
   

   
